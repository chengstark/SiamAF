{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff027a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "print(gpu_devices)\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.layers import Activation, BatchNormalization, Conv1D, Dense, GlobalAveragePooling1D, Input, MaxPooling1D, \\\n",
    "    Lambda, Dropout\n",
    "import random\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from scipy.stats import mode\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "from multiprocessing import Pool\n",
    "from resnet34 import *\n",
    "from sklearn.model_selection import KFold\n",
    "from keras import backend as K\n",
    "import faiss\n",
    "import datetime\n",
    "from itertools import product\n",
    "from annoy import AnnoyIndex\n",
    "from keras import regularizers\n",
    "from sklearn.metrics import *\n",
    "from tensorflow_model_remediation import min_diff\n",
    "\n",
    "seed_value = 1\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.compat.v1.set_random_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509642bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seed = 394\n",
    "sub_fold_data_seed = 6\n",
    "\n",
    "NOISE_RATIO = 0.0\n",
    "chosen_k = 50\n",
    "# large v1 31, mid v2 23, small v3 15\n",
    "# stanford 50\n",
    "FEATURE_LAYER_IDX = 50\n",
    "FIND_k = False\n",
    "WARM_UP_EPOCHS = 100\n",
    "RETRAIN_EPOCHS = 200\n",
    "WARMUP_FEATURE_SHAPE = 64  # THIS NEEDS TO BE CHANGED\n",
    "K_RANGE = np.concatenate((np.arange(10, 50, 10), np.arange(50, 500, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57958e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.custom_gradient\n",
    "# def grad_reverse(x):\n",
    "#     y = tf.identity(x)\n",
    "#     def custom_grad(dy):\n",
    "#         return -dy\n",
    "#     return y, custom_grad\n",
    "\n",
    "# class GradReverse(tf.keras.layers.Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         return grad_reverse(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47695282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_invariant_stanford_multitask(INPUT_LENGTH):\n",
    "    inputs = Input(shape=[INPUT_LENGTH, 1])\n",
    "\n",
    "    x = Conv1D(16,\n",
    "               kernel_size=36,\n",
    "               strides=4,\n",
    "               padding='same',\n",
    "               kernel_initializer='glorot_uniform',\n",
    "               kernel_regularizer=regularizers.l2(l=0.0001)\n",
    "               )(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=4, strides=None)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    for i in range(3):\n",
    "        x = stanford_identity_block(x, kernel_size=9, filters=32, stage=1, block=i)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=4, strides=None)(x)\n",
    "\n",
    "    x = stanford_identity_block(x, kernel_size=3, filters=64, stage=2, block=i)\n",
    "    x = stanford_identity_block(x, kernel_size=1, filters=64, stage=3, block=i)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=4, strides=None)(x)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    " \n",
    "    af_out = Dense(1, activation='sigmoid', name='af_output')(x)\n",
    "    \n",
    "    qa_x = GradReverse()(x)\n",
    "    qa_out = Dense(1, activation='sigmoid', name='qa_output')(qa_x)\n",
    "\n",
    "    m = Model(inputs, [af_out, qa_out], name='qa_invariant_stanford')\n",
    "    return m\n",
    "\n",
    "\n",
    "def qa_invariant_stanford(INPUT_LENGTH):\n",
    "    inputs = Input(shape=[INPUT_LENGTH, 1])\n",
    "\n",
    "    x = Conv1D(16,\n",
    "               kernel_size=36,\n",
    "               strides=4,\n",
    "               padding='same',\n",
    "               kernel_initializer='glorot_uniform',\n",
    "               kernel_regularizer=regularizers.l2(l=0.0001)\n",
    "               )(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=4, strides=None)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    for i in range(3):\n",
    "        x = stanford_identity_block(x, kernel_size=9, filters=32, stage=1, block=i)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=4, strides=None)(x)\n",
    "\n",
    "    x = stanford_identity_block(x, kernel_size=3, filters=64, stage=2, block=i)\n",
    "    x = stanford_identity_block(x, kernel_size=1, filters=64, stage=3, block=i)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=4, strides=None)(x)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    " \n",
    "    af_out = Dense(1, activation='sigmoid', name='af_output')(x)\n",
    "    \n",
    "    m = Model(inputs, af_out)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db807c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running', NOISE_RATIO, WARM_UP_EPOCHS)\n",
    "\n",
    "X_train = np.load('/home/chengstark/Dev/ssd/robust_learning/stanford_reboot/cheecky_reworked_data/X_train_all_{}.npy'.format(data_seed))\n",
    "X_val = np.load('/home/chengstark/Dev/ssd/robust_learning/stanford_reboot/cheecky_reworked_data/X_val_all_{}.npy'.format(data_seed))\n",
    "X_test = np.load('/home/chengstark/Dev/ssd/robust_learning/stanford_reboot/cheecky_reworked_data/X_test_all_{}.npy'.format(data_seed))\n",
    "\n",
    "y_train_af = np.load('/home/chengstark/Dev/ssd/robust_learning/stanford_reboot/cheecky_reworked_data/y_train_all_af_{}.npy'.format(data_seed))\n",
    "y_val_af = np.load('/home/chengstark/Dev/ssd/robust_learning/stanford_reboot/cheecky_reworked_data/y_val_all_af_{}.npy'.format(data_seed))\n",
    "y_test_af = np.load('/home/chengstark/Dev/ssd/robust_learning/stanford_reboot/cheecky_reworked_data/y_test_all_af_{}.npy'.format(data_seed))\n",
    "\n",
    "y_train_qa = np.load('/home/chengstark/Dev/ssd/robust_learning/stanford_reboot/cheecky_reworked_data/y_train_all_qa_{}.npy'.format(data_seed))\n",
    "y_val_qa = np.load('/home/chengstark/Dev/ssd/robust_learning/stanford_reboot/cheecky_reworked_data/y_val_all_qa_{}.npy'.format(data_seed))\n",
    "y_test_qa = np.load('/home/chengstark/Dev/ssd/robust_learning/stanford_reboot/cheecky_reworked_data/y_test_all_qa_{}.npy'.format(data_seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8eb5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_qa[y_train_qa == 1] = 0\n",
    "y_train_qa[y_train_qa == 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_label(y, ratio):\n",
    "    flip_marker = np.zeros((y.shape[0],))\n",
    "    flip_idx = np.random.choice(range(y.shape[0]), int(y.shape[0]*ratio), replace=False)\n",
    "    flip_marker[flip_idx] = 1\n",
    "    flipped_y = y.copy()\n",
    "    flipped_y[flip_idx] = 1 - flipped_y[flip_idx]\n",
    "    return flipped_y, flip_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d8a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_y_train_af, flip_marker_train = flip_label(y_train_af, NOISE_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c151db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ReverseBinaryCrossEntropy(y_true, y_pred): \n",
    "#     y_true = tf.cast(y_true, tf.float32)\n",
    "#     y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "#     y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "#     term_0 = (1 - y_true) * K.log(1 - y_pred + K.epsilon())  \n",
    "#     term_1 = y_true * K.log(y_pred + K.epsilon())\n",
    "    \n",
    "#     return K.mean(term_0 + term_1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_weight(y):\n",
    "    \n",
    "    class_weight_af = compute_class_weight('balanced', np.unique(y), y)\n",
    "    class_weight_af = {0: class_weight_af[0], 1: class_weight_af[1]}\n",
    "    sample_weight_af = np.ones(shape=(len(y),))\n",
    "    sample_weight_af[y == 1] *= class_weight_af[1]\n",
    "    sample_weight_af[y == 0] *= class_weight_af[0]\n",
    "    \n",
    "    return sample_weight_af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a45338",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "af_train_main = tf.data.Dataset.from_tensor_slices((X_train, noisy_y_train_af, get_sample_weight(noisy_y_train_af))).batch(BATCH_SIZE)\n",
    "af_train_main_qa_0 = tf.data.Dataset.from_tensor_slices((X_train[y_train_qa == 0], noisy_y_train_af[y_train_qa == 0], get_sample_weight(noisy_y_train_af[y_train_qa == 0]))).batch(BATCH_SIZE)\n",
    "af_train_main_qa_1 = tf.data.Dataset.from_tensor_slices((X_train[y_train_qa == 1], noisy_y_train_af[y_train_qa == 1], get_sample_weight(noisy_y_train_af[y_train_qa == 1]))).batch(BATCH_SIZE)\n",
    "\n",
    "af_val_main = tf.data.Dataset.from_tensor_slices((X_val, y_val_af)).batch(BATCH_SIZE)\n",
    "af_val_main_qa_0 = tf.data.Dataset.from_tensor_slices((X_val[y_val_qa == 0], y_val_af[y_val_qa == 0])).batch(BATCH_SIZE)\n",
    "af_val_main_qa_1 = tf.data.Dataset.from_tensor_slices((X_val[y_val_qa == 1], y_val_af[y_val_qa == 1])).batch(BATCH_SIZE)\n",
    "\n",
    "train_dataset = min_diff.keras.utils.input_utils.pack_min_diff_data(af_train_main, af_train_main_qa_0, af_train_main_qa_1)\n",
    "val_dataset = min_diff.keras.utils.input_utils.pack_min_diff_data(af_val_main, af_val_main_qa_0, af_val_main_qa_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd45bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "\n",
    "min_diff_loss = min_diff.losses.MMDLoss()\n",
    "min_diff_weight = 10.0\n",
    "\n",
    "qa_invariant_model = qa_invariant_stanford(INPUT_LENGTH=X_train.shape[1])\n",
    "\n",
    "min_diff_model = min_diff.keras.MinDiffModel(qa_invariant_model, min_diff_loss, min_diff_weight)\n",
    "\n",
    "min_diff_model.compile(\n",
    "    optimizer=Adam(lr=LR), loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c26152",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qa_invariant_cp = 'qa_invariant_{}_{}_{}.h5'.format(WARM_UP_EPOCHS, NOISE_RATIO, data_seed)\n",
    "qa_invariant_cp\n",
    "\n",
    "hist = min_diff_model.fit(\n",
    "    train_dataset,    \n",
    "    callbacks=[\n",
    "        ModelCheckpoint(filepath=qa_invariant_cp, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "    ],\n",
    "    \n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_invariant_model.load_weights(qa_invariant_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = qa_invariant_model.predict(X_test)\n",
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26154a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "af_test_pred = test_pred.squeeze()\n",
    "af_test_pred_binary = af_test_pred.copy()\n",
    "af_test_pred_binary[af_test_pred_binary > 0.5] = 1\n",
    "af_test_pred_binary[af_test_pred_binary <= 0.5] = 0\n",
    "af_test_f1 = f1_score(y_test_af, af_test_pred_binary)\n",
    "af_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa45dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bce = tf.keras.losses.BinaryCrossentropy()\n",
    "# a = tf.Variable(np.asarray([1, 0, 0, 1]), dtype=tf.float32)\n",
    "# b = 1 - a\n",
    "# c = a\n",
    "# d = tf.Variable(np.asarray([0, 0, 0, 0]), dtype=tf.float32)\n",
    "# print(bce(a, b))\n",
    "# print(bce(a, c))\n",
    "# print(bce(a, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db108a",
   "metadata": {},
   "source": [
    "# PacMap visualization warmup feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd689d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pacmap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ee62a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, layer in enumerate(qa_invariant_model.layers):\n",
    "    print(idx, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce59a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_extractor = Model(qa_invariant_model.input, qa_invariant_model.layers[FEATURE_LAYER_IDX].output)\n",
    "feats = feat_extractor.predict(X_train)\n",
    "feats = feats - np.mean(feats, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17343dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597fb106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_annoy(warm_up_model_path, X_train, feature_layer_idx, save=True, ANNOY_NAME=None):\n",
    "    best_warm_up = load_model(warm_up_model_path, custom_objects={'GradReverse': GradReverse})\n",
    "    warm_up_feat_extractor = Model(best_warm_up.input, best_warm_up.layers[feature_layer_idx].output)\n",
    "    warm_up_feats_raw = warm_up_feat_extractor.predict(X_train)\n",
    "    warm_up_feats = warm_up_feats_raw - np.mean(warm_up_feats_raw, axis=0)\n",
    "    f = warm_up_feats.shape[1]\n",
    "    a = AnnoyIndex(f, 'angular')\n",
    "\n",
    "    if ANNOY_NAME is None:\n",
    "        ANNOY_NAME_ = 'stanford_all_{}_{}_{}_{}_mean_center.ann'.format(NOISE_RATIO, data_seed, 'qa_invariant', WARM_UP_EPOCHS)\n",
    "    else:\n",
    "        ANNOY_NAME_ = ANNOY_NAME\n",
    "\n",
    "    if os.path.exists(ANNOY_NAME_):\n",
    "        a.load(ANNOY_NAME_)\n",
    "        print('Annoy loaded from', ANNOY_NAME_)\n",
    "    else:\n",
    "        print('Building AnnoyIndex for warm up feats of shape {}'.format(warm_up_feats.shape))\n",
    "        a.set_seed(1)\n",
    "        for idx, feat in enumerate(tqdm(warm_up_feats)):\n",
    "            a.add_item(idx, feat)\n",
    "\n",
    "        a.build(1000, n_jobs=15)  # 10 trees\n",
    "        if save:\n",
    "            a.save(ANNOY_NAME_)\n",
    "            print('Annoy saved to', ANNOY_NAME_)\n",
    "    del warm_up_feat_extractor, warm_up_feats\n",
    "    gc.collect()\n",
    "\n",
    "    return a\n",
    "\n",
    "def build_annoy_with_model(warm_up_feat_extractor, X_train, feature_layer_idx, save=True, ANNOY_NAME=None):\n",
    "    warm_up_feats_raw = warm_up_feat_extractor.predict(X_train)\n",
    "    warm_up_feats = warm_up_feats_raw - np.mean(warm_up_feats_raw, axis=0)\n",
    "    f = warm_up_feats.shape[1]\n",
    "    a = AnnoyIndex(f, 'angular')\n",
    "\n",
    "    if ANNOY_NAME is None:\n",
    "        ANNOY_NAME_ = 'stanford_all_{}_{}_{}_{}_mean_center.ann'.format(NOISE_RATIO, data_seed, 'qa_invariant', WARM_UP_EPOCHS)\n",
    "    else:\n",
    "        ANNOY_NAME_ = ANNOY_NAME\n",
    "\n",
    "    if os.path.exists(ANNOY_NAME_):\n",
    "        a.load(ANNOY_NAME_)\n",
    "        print('Annoy loaded from', ANNOY_NAME_)\n",
    "    else:\n",
    "        print('Building AnnoyIndex for warm up feats of shape {}'.format(warm_up_feats.shape))\n",
    "        a.set_seed(1)\n",
    "        for idx, feat in enumerate(tqdm(warm_up_feats)):\n",
    "            a.add_item(idx, feat)\n",
    "\n",
    "        a.build(1000, n_jobs=15)  # 10 trees\n",
    "        if save:\n",
    "            a.save(ANNOY_NAME_)\n",
    "            print('Annoy saved to', ANNOY_NAME_)\n",
    "    del warm_up_feat_extractor, warm_up_feats\n",
    "    gc.collect()\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "annoy_index = build_annoy_with_model(feat_extractor, X_train, FEATURE_LAYER_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'qa_invariant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing.pool as mpp\n",
    "\n",
    "def istarmap(self, func, iterable, chunksize=1):\n",
    "    \"\"\"starmap-version of imap\n",
    "    \"\"\"\n",
    "    if self._state != mpp.RUN:WARM_UP_EPOCHS \n",
    "        raise ValueError(\"Pool not running\")\n",
    "\n",
    "    if chunksize < 1:\n",
    "        raise ValueError(\n",
    "            \"Chunksize must be 1+, not {0:n}\".format(\n",
    "                chunksize))\n",
    "\n",
    "    task_batches = mpp.Pool._get_tasks(func, iterable, chunksize)\n",
    "    result = mpp.IMapIterator(self._cache)\n",
    "    self._taskqueue.put(\n",
    "        (\n",
    "            self._guarded_task_generation(result._job,\n",
    "                                          mpp.starmapstar,\n",
    "                                          task_batches),\n",
    "            result._set_length\n",
    "        ))\n",
    "    return (item for chunk in result for item in chunk)\n",
    "\n",
    "def annoy_pacmap_prepare_child(idx_t, annoy_index_path, f):\n",
    "\n",
    "    annoy_index = AnnoyIndex(f, 'angular')\n",
    "    annoy_index.load(annoy_index_path)\n",
    "    return idx_t, annoy_index.get_nns_by_item(idx_t, 1000 + 1)\n",
    "\n",
    "\n",
    "def annoy_pacmap_prepare(feats, annoy_index):\n",
    "    \n",
    "    tmp_annoy_index_path = 'tmp_annoy_index_{}_{}.ann'.format(NOISE_RATIO, data_seed)\n",
    "    f = WARMUP_FEATURE_SHAPE\n",
    "    annoy_index.save(tmp_annoy_index_path)\n",
    "\n",
    "    pool_args = []\n",
    "    for idx_t in range(len(X_train)):\n",
    "        pool_args.append([idx_t, tmp_annoy_index_path, f])\n",
    "    \n",
    "    n, dim = feats.shape\n",
    "    nbrs = np.zeros((n, 1000), dtype=np.int32)\n",
    "    \n",
    "    mpp.Pool.istarmap = istarmap\n",
    "    pool = Pool(processes=15)\n",
    "\n",
    "    for rst in tqdm(pool.istarmap(annoy_pacmap_prepare_child, pool_args), total=len(pool_args)):\n",
    "        nbrs[rst[0], :] = rst[1][1:]\n",
    "\n",
    "    pool.terminate()\n",
    "    \n",
    "    return nbrs\n",
    "\n",
    "\n",
    "\n",
    "nbrs = annoy_pacmap_prepare(feats, annoy_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pacmap_nbrs_{}_{}_{}_{}_mean_center.npy'.format(1000, WARM_UP_EPOCHS, NOISE_RATIO, data_seed), nbrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b78047",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = chosen_k\n",
    "n, dim = feats.shape\n",
    "\n",
    "scaled_dist = np.ones((n, n_neighbors)) # No scaling is needed\n",
    "scaled_dist = scaled_dist.astype(np.float32)\n",
    "\n",
    "pair_neighbors = pacmap.sample_neighbors_pair(feats.astype(np.float32), scaled_dist, nbrs, np.int32(n_neighbors))\n",
    "\n",
    "embedding = pacmap.PaCMAP(n_dims=2, n_neighbors=n_neighbors, MN_ratio=0.5, FP_ratio=2.0, pair_neighbors=pair_neighbors)\n",
    "X_transformed = embedding.fit_transform(feats, init=\"pca\")\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21faf249",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pacmap_transformed_{}_{}_{}_{}_mean_center.npy'.format(1000, WARM_UP_EPOCHS, NOISE_RATIO, data_seed), X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6cb261",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X_transformed[:, 0][y_train_af==0], X_transformed[:, 1][y_train_af==0], label='Clean Non-AF', s=1, alpha=0.5, c='tab:blue')\n",
    "plt.scatter(X_transformed[:, 0][y_train_af==1], X_transformed[:, 1][y_train_af==1], label='Clean AF', s=1, alpha=0.5, c='tab:red')\n",
    "plt.legend(markerscale=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X_transformed[:, 0][noisy_y_train_af==0], X_transformed[:, 1][noisy_y_train_af==0], label='Noisy Non-AF', s=1, alpha=0.1, c='tab:blue')\n",
    "plt.scatter(X_transformed[:, 0][noisy_y_train_af==1], X_transformed[:, 1][noisy_y_train_af==1], label='Noisy AF', s=1, alpha=0.1, c='tab:red')\n",
    "plt.legend(markerscale=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0341234",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "# plt.scatter(X_transformed[:, 0][y_train_qa==0], X_transformed[:, 1][y_train_qa==0], label='Noisy Non-AF', s=1, alpha=0.5, c='tab:blue')\n",
    "plt.scatter(X_transformed[:, 0][y_train_qa==1], X_transformed[:, 1][y_train_qa==1], label='Noisy AF', s=1, alpha=0.5, c='tab:red')\n",
    "plt.legend(markerscale=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485cb98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X_transformed[:, 0][y_train_qa==0], X_transformed[:, 1][y_train_qa==0], label='Noisy Non-AF', s=1, alpha=0.5, c='tab:blue')\n",
    "# plt.scatter(X_transformed[:, 0][y_train_qa==1], X_transformed[:, 1][y_train_qa==1], label='Noisy AF', s=1, alpha=0.5, c='tab:red')\n",
    "plt.legend(markerscale=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_qa = np.load('/home/chengstark/Dev/ssd/robust_learning/stanford_reboot/cheecky_reworked_data/y_train_all_qa_{}.npy'.format(data_seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X_transformed[:, 0][y_train_qa==1], X_transformed[:, 1][y_train_qa==1], label='Acceptable', s=1, alpha=0.1, c='tab:orange')\n",
    "plt.scatter(X_transformed[:, 0][y_train_qa==2], X_transformed[:, 1][y_train_qa==2], label='Good', s=1, alpha=0.1, c='tab:blue')\n",
    "plt.scatter(X_transformed[:, 0][y_train_qa==0], X_transformed[:, 1][y_train_qa==0], label='Bad', s=1, alpha=0.1, c='tab:red')\n",
    "\n",
    "plt.legend(markerscale=10)\n",
    "plt.show()\n",
    "\n",
    "print(np.unique(y_train_qa, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X_transformed[:, 0][y_train_qa==1], X_transformed[:, 1][y_train_qa==1], label='Acceptable', s=1, alpha=0.1, c='tab:orange')\n",
    "# plt.scatter(X_transformed[:, 0][y_train_qa==2], X_transformed[:, 1][y_train_qa==2], label='Good', s=1, alpha=0.1, c='tab:blue')\n",
    "plt.scatter(X_transformed[:, 0][y_train_qa==0], X_transformed[:, 1][y_train_qa==0], label='Bad', s=1, alpha=0.1, c='tab:red')\n",
    "\n",
    "plt.legend(markerscale=10)\n",
    "plt.show()\n",
    "\n",
    "print(np.unique(y_train_qa, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785217f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "# plt.scatter(X_transformed[:, 0][y_train_qa==1], X_transformed[:, 1][y_train_qa==1], label='Acceptable', s=1, alpha=0.1, c='tab:orange')\n",
    "plt.scatter(X_transformed[:, 0][y_train_qa==2], X_transformed[:, 1][y_train_qa==2], label='Good', s=1, alpha=0.1, c='tab:blue')\n",
    "# plt.scatter(X_transformed[:, 0][y_train_qa==0], X_transformed[:, 1][y_train_qa==0], label='Bad', s=1, alpha=0.1, c='tab:red')\n",
    "\n",
    "plt.legend(markerscale=10)\n",
    "plt.show()\n",
    "\n",
    "print(np.unique(y_train_qa, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63b86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365db74a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
